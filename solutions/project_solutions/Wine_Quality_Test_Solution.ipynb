{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 3: Wine Quality Test Solution",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YinhaoHe/Python-AI-teaching/blob/master/Project_3_Wine_Quality_Test_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k7heYxC5-HX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "efde502d-a14a-4ee9-de72-1c0af0c2e588"
      },
      "source": [
        "!mkdir .kaggle\n",
        "!mkdir ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owT6P1qwtmpx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8a5a871f-18ec-42ab-c20d-29ece2100108"
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "token = {\"username\":\"bdonluong\",\"key\":\"44600f6f95296c477b56c17f51c39d6f\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "    \n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!kaggle config set -n path -v{/content}\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "- path is now set to: {/content}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZSWZ7Zu81Km",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "053e491f-d9c6-4924-b1fd-d5e8134b029e"
      },
      "source": [
        "!kaggle datasets download -d ardamavi/sign-language-digits-dataset -p /content/datasets/kaggle-hs\n",
        "%cd datasets/kaggle-hs/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sign-language-digits-dataset.zip to /content/datasets/kaggle-hs\n",
            " 63% 5.00M/7.94M [00:00<00:00, 35.4MB/s]\n",
            "100% 7.94M/7.94M [00:00<00:00, 38.9MB/s]\n",
            "/content/datasets/kaggle-hs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b2KKIVV_UNG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "11123fd5-50a9-4e88-f5cc-434842840f9f"
      },
      "source": [
        "!unzip \\*zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  Sign-language-digits-dataset.zip\n",
            "  inflating: X.npy                   \n",
            "  inflating: Y.npy                   \n",
            "\n",
            "Archive:  sign-language-digits-dataset.zip\n",
            "replace Sign-language-digits-dataset.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: Sign-language-digits-dataset.zip  \n",
            "\n",
            "2 archives were successfully processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8Iv3Wvq5sxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d88a6880-63ca-48a6-b22e-481ec935ce6d"
      },
      "source": [
        "dsroot = os.getcwd()\n",
        "os.listdir(dsroot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sign-language-digits-dataset.zip',\n",
              " 'Y.npy',\n",
              " 'X.npy',\n",
              " 'sign-language-digits-dataset.zip']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzMX2TN6A5O1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "df = pd.read_csv(os.path.join(dsroot, \"winequality-red.csv\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH58BAzQDSsw",
        "colab_type": "text"
      },
      "source": [
        "Print the dataframe. Describe the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOH-drBGq5k8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1142
        },
        "outputId": "1962e9e4-f613-41a5-91db-58d3d485c076"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  alcohol  quality\n",
            "0               7.4             0.700         0.00             1.9      0.076                 11.0                  34.0  0.99780  3.51       0.56      9.4        5\n",
            "1               7.8             0.880         0.00             2.6      0.098                 25.0                  67.0  0.99680  3.20       0.68      9.8        5\n",
            "2               7.8             0.760         0.04             2.3      0.092                 15.0                  54.0  0.99700  3.26       0.65      9.8        5\n",
            "3              11.2             0.280         0.56             1.9      0.075                 17.0                  60.0  0.99800  3.16       0.58      9.8        6\n",
            "4               7.4             0.700         0.00             1.9      0.076                 11.0                  34.0  0.99780  3.51       0.56      9.4        5\n",
            "5               7.4             0.660         0.00             1.8      0.075                 13.0                  40.0  0.99780  3.51       0.56      9.4        5\n",
            "6               7.9             0.600         0.06             1.6      0.069                 15.0                  59.0  0.99640  3.30       0.46      9.4        5\n",
            "7               7.3             0.650         0.00             1.2      0.065                 15.0                  21.0  0.99460  3.39       0.47     10.0        7\n",
            "8               7.8             0.580         0.02             2.0      0.073                  9.0                  18.0  0.99680  3.36       0.57      9.5        7\n",
            "9               7.5             0.500         0.36             6.1      0.071                 17.0                 102.0  0.99780  3.35       0.80     10.5        5\n",
            "10              6.7             0.580         0.08             1.8      0.097                 15.0                  65.0  0.99590  3.28       0.54      9.2        5\n",
            "11              7.5             0.500         0.36             6.1      0.071                 17.0                 102.0  0.99780  3.35       0.80     10.5        5\n",
            "12              5.6             0.615         0.00             1.6      0.089                 16.0                  59.0  0.99430  3.58       0.52      9.9        5\n",
            "13              7.8             0.610         0.29             1.6      0.114                  9.0                  29.0  0.99740  3.26       1.56      9.1        5\n",
            "14              8.9             0.620         0.18             3.8      0.176                 52.0                 145.0  0.99860  3.16       0.88      9.2        5\n",
            "15              8.9             0.620         0.19             3.9      0.170                 51.0                 148.0  0.99860  3.17       0.93      9.2        5\n",
            "16              8.5             0.280         0.56             1.8      0.092                 35.0                 103.0  0.99690  3.30       0.75     10.5        7\n",
            "17              8.1             0.560         0.28             1.7      0.368                 16.0                  56.0  0.99680  3.11       1.28      9.3        5\n",
            "18              7.4             0.590         0.08             4.4      0.086                  6.0                  29.0  0.99740  3.38       0.50      9.0        4\n",
            "19              7.9             0.320         0.51             1.8      0.341                 17.0                  56.0  0.99690  3.04       1.08      9.2        6\n",
            "20              8.9             0.220         0.48             1.8      0.077                 29.0                  60.0  0.99680  3.39       0.53      9.4        6\n",
            "21              7.6             0.390         0.31             2.3      0.082                 23.0                  71.0  0.99820  3.52       0.65      9.7        5\n",
            "22              7.9             0.430         0.21             1.6      0.106                 10.0                  37.0  0.99660  3.17       0.91      9.5        5\n",
            "23              8.5             0.490         0.11             2.3      0.084                  9.0                  67.0  0.99680  3.17       0.53      9.4        5\n",
            "24              6.9             0.400         0.14             2.4      0.085                 21.0                  40.0  0.99680  3.43       0.63      9.7        6\n",
            "25              6.3             0.390         0.16             1.4      0.080                 11.0                  23.0  0.99550  3.34       0.56      9.3        5\n",
            "26              7.6             0.410         0.24             1.8      0.080                  4.0                  11.0  0.99620  3.28       0.59      9.5        5\n",
            "27              7.9             0.430         0.21             1.6      0.106                 10.0                  37.0  0.99660  3.17       0.91      9.5        5\n",
            "28              7.1             0.710         0.00             1.9      0.080                 14.0                  35.0  0.99720  3.47       0.55      9.4        5\n",
            "29              7.8             0.645         0.00             2.0      0.082                  8.0                  16.0  0.99640  3.38       0.59      9.8        6\n",
            "...             ...               ...          ...             ...        ...                  ...                   ...      ...   ...        ...      ...      ...\n",
            "1569            6.2             0.510         0.14             1.9      0.056                 15.0                  34.0  0.99396  3.48       0.57     11.5        6\n",
            "1570            6.4             0.360         0.53             2.2      0.230                 19.0                  35.0  0.99340  3.37       0.93     12.4        6\n",
            "1571            6.4             0.380         0.14             2.2      0.038                 15.0                  25.0  0.99514  3.44       0.65     11.1        6\n",
            "1572            7.3             0.690         0.32             2.2      0.069                 35.0                 104.0  0.99632  3.33       0.51      9.5        5\n",
            "1573            6.0             0.580         0.20             2.4      0.075                 15.0                  50.0  0.99467  3.58       0.67     12.5        6\n",
            "1574            5.6             0.310         0.78            13.9      0.074                 23.0                  92.0  0.99677  3.39       0.48     10.5        6\n",
            "1575            7.5             0.520         0.40             2.2      0.060                 12.0                  20.0  0.99474  3.26       0.64     11.8        6\n",
            "1576            8.0             0.300         0.63             1.6      0.081                 16.0                  29.0  0.99588  3.30       0.78     10.8        6\n",
            "1577            6.2             0.700         0.15             5.1      0.076                 13.0                  27.0  0.99622  3.54       0.60     11.9        6\n",
            "1578            6.8             0.670         0.15             1.8      0.118                 13.0                  20.0  0.99540  3.42       0.67     11.3        6\n",
            "1579            6.2             0.560         0.09             1.7      0.053                 24.0                  32.0  0.99402  3.54       0.60     11.3        5\n",
            "1580            7.4             0.350         0.33             2.4      0.068                  9.0                  26.0  0.99470  3.36       0.60     11.9        6\n",
            "1581            6.2             0.560         0.09             1.7      0.053                 24.0                  32.0  0.99402  3.54       0.60     11.3        5\n",
            "1582            6.1             0.715         0.10             2.6      0.053                 13.0                  27.0  0.99362  3.57       0.50     11.9        5\n",
            "1583            6.2             0.460         0.29             2.1      0.074                 32.0                  98.0  0.99578  3.33       0.62      9.8        5\n",
            "1584            6.7             0.320         0.44             2.4      0.061                 24.0                  34.0  0.99484  3.29       0.80     11.6        7\n",
            "1585            7.2             0.390         0.44             2.6      0.066                 22.0                  48.0  0.99494  3.30       0.84     11.5        6\n",
            "1586            7.5             0.310         0.41             2.4      0.065                 34.0                  60.0  0.99492  3.34       0.85     11.4        6\n",
            "1587            5.8             0.610         0.11             1.8      0.066                 18.0                  28.0  0.99483  3.55       0.66     10.9        6\n",
            "1588            7.2             0.660         0.33             2.5      0.068                 34.0                 102.0  0.99414  3.27       0.78     12.8        6\n",
            "1589            6.6             0.725         0.20             7.8      0.073                 29.0                  79.0  0.99770  3.29       0.54      9.2        5\n",
            "1590            6.3             0.550         0.15             1.8      0.077                 26.0                  35.0  0.99314  3.32       0.82     11.6        6\n",
            "1591            5.4             0.740         0.09             1.7      0.089                 16.0                  26.0  0.99402  3.67       0.56     11.6        6\n",
            "1592            6.3             0.510         0.13             2.3      0.076                 29.0                  40.0  0.99574  3.42       0.75     11.0        6\n",
            "1593            6.8             0.620         0.08             1.9      0.068                 28.0                  38.0  0.99651  3.42       0.82      9.5        6\n",
            "1594            6.2             0.600         0.08             2.0      0.090                 32.0                  44.0  0.99490  3.45       0.58     10.5        5\n",
            "1595            5.9             0.550         0.10             2.2      0.062                 39.0                  51.0  0.99512  3.52       0.76     11.2        6\n",
            "1596            6.3             0.510         0.13             2.3      0.076                 29.0                  40.0  0.99574  3.42       0.75     11.0        6\n",
            "1597            5.9             0.645         0.12             2.0      0.075                 32.0                  44.0  0.99547  3.57       0.71     10.2        5\n",
            "1598            6.0             0.310         0.47             3.6      0.067                 18.0                  42.0  0.99549  3.39       0.66     11.0        6\n",
            "\n",
            "[1599 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sBH_98bFsy7",
        "colab_type": "text"
      },
      "source": [
        "**Data Processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSI4sagxC7Nd",
        "colab_type": "text"
      },
      "source": [
        "Replace the quality column with a column named \"pass\".\n",
        "If the quality is >= 7 then the wine passes, otherwise it fails."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIRpVR-61lzd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "4cace479-8eb5-41a0-83cf-c6d17f4305f1"
      },
      "source": [
        "df.insert(12,\"pass\",0)\n",
        "for i in range(df[\"pass\"].size):\n",
        "  if df[\"quality\"][i] >= 7:\n",
        "    df[\"pass\"][i] = 1\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-f6be54f2136a>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    for i in range(df[])\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIylmX8YCxiA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1105
        },
        "outputId": "b4ca1bcb-e5ed-4a6a-dbac-04317c4f0fb3"
      },
      "source": [
        "wine_pass = list()\n",
        "for quality in df[\"quality\"]:\n",
        "  if quality >= 7:\n",
        "    wine_pass.append(1)\n",
        "  else:\n",
        "    wine_pass.append(0)\n",
        "df.insert(12,\"pass\", wine_pass)\n",
        "df = df.drop(\"quality\", axis = 1)\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  alcohol  pass\n",
            "0               7.4             0.700         0.00             1.9      0.076                 11.0                  34.0  0.99780  3.51       0.56      9.4     0\n",
            "1               7.8             0.880         0.00             2.6      0.098                 25.0                  67.0  0.99680  3.20       0.68      9.8     0\n",
            "2               7.8             0.760         0.04             2.3      0.092                 15.0                  54.0  0.99700  3.26       0.65      9.8     0\n",
            "3              11.2             0.280         0.56             1.9      0.075                 17.0                  60.0  0.99800  3.16       0.58      9.8     0\n",
            "4               7.4             0.700         0.00             1.9      0.076                 11.0                  34.0  0.99780  3.51       0.56      9.4     0\n",
            "5               7.4             0.660         0.00             1.8      0.075                 13.0                  40.0  0.99780  3.51       0.56      9.4     0\n",
            "6               7.9             0.600         0.06             1.6      0.069                 15.0                  59.0  0.99640  3.30       0.46      9.4     0\n",
            "7               7.3             0.650         0.00             1.2      0.065                 15.0                  21.0  0.99460  3.39       0.47     10.0     1\n",
            "8               7.8             0.580         0.02             2.0      0.073                  9.0                  18.0  0.99680  3.36       0.57      9.5     1\n",
            "9               7.5             0.500         0.36             6.1      0.071                 17.0                 102.0  0.99780  3.35       0.80     10.5     0\n",
            "10              6.7             0.580         0.08             1.8      0.097                 15.0                  65.0  0.99590  3.28       0.54      9.2     0\n",
            "11              7.5             0.500         0.36             6.1      0.071                 17.0                 102.0  0.99780  3.35       0.80     10.5     0\n",
            "12              5.6             0.615         0.00             1.6      0.089                 16.0                  59.0  0.99430  3.58       0.52      9.9     0\n",
            "13              7.8             0.610         0.29             1.6      0.114                  9.0                  29.0  0.99740  3.26       1.56      9.1     0\n",
            "14              8.9             0.620         0.18             3.8      0.176                 52.0                 145.0  0.99860  3.16       0.88      9.2     0\n",
            "15              8.9             0.620         0.19             3.9      0.170                 51.0                 148.0  0.99860  3.17       0.93      9.2     0\n",
            "16              8.5             0.280         0.56             1.8      0.092                 35.0                 103.0  0.99690  3.30       0.75     10.5     1\n",
            "17              8.1             0.560         0.28             1.7      0.368                 16.0                  56.0  0.99680  3.11       1.28      9.3     0\n",
            "18              7.4             0.590         0.08             4.4      0.086                  6.0                  29.0  0.99740  3.38       0.50      9.0     0\n",
            "19              7.9             0.320         0.51             1.8      0.341                 17.0                  56.0  0.99690  3.04       1.08      9.2     0\n",
            "20              8.9             0.220         0.48             1.8      0.077                 29.0                  60.0  0.99680  3.39       0.53      9.4     0\n",
            "21              7.6             0.390         0.31             2.3      0.082                 23.0                  71.0  0.99820  3.52       0.65      9.7     0\n",
            "22              7.9             0.430         0.21             1.6      0.106                 10.0                  37.0  0.99660  3.17       0.91      9.5     0\n",
            "23              8.5             0.490         0.11             2.3      0.084                  9.0                  67.0  0.99680  3.17       0.53      9.4     0\n",
            "24              6.9             0.400         0.14             2.4      0.085                 21.0                  40.0  0.99680  3.43       0.63      9.7     0\n",
            "25              6.3             0.390         0.16             1.4      0.080                 11.0                  23.0  0.99550  3.34       0.56      9.3     0\n",
            "26              7.6             0.410         0.24             1.8      0.080                  4.0                  11.0  0.99620  3.28       0.59      9.5     0\n",
            "27              7.9             0.430         0.21             1.6      0.106                 10.0                  37.0  0.99660  3.17       0.91      9.5     0\n",
            "28              7.1             0.710         0.00             1.9      0.080                 14.0                  35.0  0.99720  3.47       0.55      9.4     0\n",
            "29              7.8             0.645         0.00             2.0      0.082                  8.0                  16.0  0.99640  3.38       0.59      9.8     0\n",
            "...             ...               ...          ...             ...        ...                  ...                   ...      ...   ...        ...      ...   ...\n",
            "1569            6.2             0.510         0.14             1.9      0.056                 15.0                  34.0  0.99396  3.48       0.57     11.5     0\n",
            "1570            6.4             0.360         0.53             2.2      0.230                 19.0                  35.0  0.99340  3.37       0.93     12.4     0\n",
            "1571            6.4             0.380         0.14             2.2      0.038                 15.0                  25.0  0.99514  3.44       0.65     11.1     0\n",
            "1572            7.3             0.690         0.32             2.2      0.069                 35.0                 104.0  0.99632  3.33       0.51      9.5     0\n",
            "1573            6.0             0.580         0.20             2.4      0.075                 15.0                  50.0  0.99467  3.58       0.67     12.5     0\n",
            "1574            5.6             0.310         0.78            13.9      0.074                 23.0                  92.0  0.99677  3.39       0.48     10.5     0\n",
            "1575            7.5             0.520         0.40             2.2      0.060                 12.0                  20.0  0.99474  3.26       0.64     11.8     0\n",
            "1576            8.0             0.300         0.63             1.6      0.081                 16.0                  29.0  0.99588  3.30       0.78     10.8     0\n",
            "1577            6.2             0.700         0.15             5.1      0.076                 13.0                  27.0  0.99622  3.54       0.60     11.9     0\n",
            "1578            6.8             0.670         0.15             1.8      0.118                 13.0                  20.0  0.99540  3.42       0.67     11.3     0\n",
            "1579            6.2             0.560         0.09             1.7      0.053                 24.0                  32.0  0.99402  3.54       0.60     11.3     0\n",
            "1580            7.4             0.350         0.33             2.4      0.068                  9.0                  26.0  0.99470  3.36       0.60     11.9     0\n",
            "1581            6.2             0.560         0.09             1.7      0.053                 24.0                  32.0  0.99402  3.54       0.60     11.3     0\n",
            "1582            6.1             0.715         0.10             2.6      0.053                 13.0                  27.0  0.99362  3.57       0.50     11.9     0\n",
            "1583            6.2             0.460         0.29             2.1      0.074                 32.0                  98.0  0.99578  3.33       0.62      9.8     0\n",
            "1584            6.7             0.320         0.44             2.4      0.061                 24.0                  34.0  0.99484  3.29       0.80     11.6     1\n",
            "1585            7.2             0.390         0.44             2.6      0.066                 22.0                  48.0  0.99494  3.30       0.84     11.5     0\n",
            "1586            7.5             0.310         0.41             2.4      0.065                 34.0                  60.0  0.99492  3.34       0.85     11.4     0\n",
            "1587            5.8             0.610         0.11             1.8      0.066                 18.0                  28.0  0.99483  3.55       0.66     10.9     0\n",
            "1588            7.2             0.660         0.33             2.5      0.068                 34.0                 102.0  0.99414  3.27       0.78     12.8     0\n",
            "1589            6.6             0.725         0.20             7.8      0.073                 29.0                  79.0  0.99770  3.29       0.54      9.2     0\n",
            "1590            6.3             0.550         0.15             1.8      0.077                 26.0                  35.0  0.99314  3.32       0.82     11.6     0\n",
            "1591            5.4             0.740         0.09             1.7      0.089                 16.0                  26.0  0.99402  3.67       0.56     11.6     0\n",
            "1592            6.3             0.510         0.13             2.3      0.076                 29.0                  40.0  0.99574  3.42       0.75     11.0     0\n",
            "1593            6.8             0.620         0.08             1.9      0.068                 28.0                  38.0  0.99651  3.42       0.82      9.5     0\n",
            "1594            6.2             0.600         0.08             2.0      0.090                 32.0                  44.0  0.99490  3.45       0.58     10.5     0\n",
            "1595            5.9             0.550         0.10             2.2      0.062                 39.0                  51.0  0.99512  3.52       0.76     11.2     0\n",
            "1596            6.3             0.510         0.13             2.3      0.076                 29.0                  40.0  0.99574  3.42       0.75     11.0     0\n",
            "1597            5.9             0.645         0.12             2.0      0.075                 32.0                  44.0  0.99547  3.57       0.71     10.2     0\n",
            "1598            6.0             0.310         0.47             3.6      0.067                 18.0                  42.0  0.99549  3.39       0.66     11.0     0\n",
            "\n",
            "[1599 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhtmGAD2E7wJ",
        "colab_type": "text"
      },
      "source": [
        "Check if there are any missing values (NaN or 0's that dont make sense). If there are remove them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkrTwjfWFeKi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "5548d832-64b9-4083-fb75-ff23540524d3"
      },
      "source": [
        "print(df.describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       fixed acidity  volatile acidity  citric acid  residual sugar    chlorides  free sulfur dioxide  total sulfur dioxide      density           pH    sulphates      alcohol         pass\n",
            "count    1599.000000       1599.000000  1599.000000     1599.000000  1599.000000          1599.000000           1599.000000  1599.000000  1599.000000  1599.000000  1599.000000  1599.000000\n",
            "mean        8.319637          0.527821     0.270976        2.538806     0.087467            15.874922             46.467792     0.996747     3.311113     0.658149    10.422983     0.135710\n",
            "std         1.741096          0.179060     0.194801        1.409928     0.047065            10.460157             32.895324     0.001887     0.154386     0.169507     1.065668     0.342587\n",
            "min         4.600000          0.120000     0.000000        0.900000     0.012000             1.000000              6.000000     0.990070     2.740000     0.330000     8.400000     0.000000\n",
            "25%         7.100000          0.390000     0.090000        1.900000     0.070000             7.000000             22.000000     0.995600     3.210000     0.550000     9.500000     0.000000\n",
            "50%         7.900000          0.520000     0.260000        2.200000     0.079000            14.000000             38.000000     0.996750     3.310000     0.620000    10.200000     0.000000\n",
            "75%         9.200000          0.640000     0.420000        2.600000     0.090000            21.000000             62.000000     0.997835     3.400000     0.730000    11.100000     0.000000\n",
            "max        15.900000          1.580000     1.000000       15.500000     0.611000            72.000000            289.000000     1.003690     4.010000     2.000000    14.900000     1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tSKrPjKFnFq",
        "colab_type": "text"
      },
      "source": [
        "**Data Standardization**\n",
        "Look at your code from yesterday. I want you to standardize this data in the same way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR-leNrSF8Tz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1105
        },
        "outputId": "94093efc-61ee-46c4-f4d7-1e0e38aa0eb7"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "df_scaled = preprocessing.scale(df)\n",
        "df_scaled = pd.DataFrame(df_scaled, columns=df.columns)\n",
        "df_scaled['pass'] = df['pass']\n",
        "df = df_scaled\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide   density        pH  sulphates   alcohol  pass\n",
            "0         -0.528360          0.961877    -1.391472       -0.453218  -0.243707            -0.466193             -0.379133  0.558274  1.288643  -0.579207 -0.960246     0\n",
            "1         -0.298547          1.967442    -1.391472        0.043416   0.223875             0.872638              0.624363  0.028261 -0.719933   0.128950 -0.584777     0\n",
            "2         -0.298547          1.297065    -1.186070       -0.169427   0.096353            -0.083669              0.229047  0.134264 -0.331177  -0.048089 -0.584777     0\n",
            "3          1.654856         -1.384443     1.484154       -0.453218  -0.264960             0.107592              0.411500  0.664277 -0.979104  -0.461180 -0.584777     0\n",
            "4         -0.528360          0.961877    -1.391472       -0.453218  -0.243707            -0.466193             -0.379133  0.558274  1.288643  -0.579207 -0.960246     0\n",
            "5         -0.528360          0.738418    -1.391472       -0.524166  -0.264960            -0.274931             -0.196679  0.558274  1.288643  -0.579207 -0.960246     0\n",
            "6         -0.241094          0.403229    -1.083370       -0.666062  -0.392483            -0.083669              0.381091 -0.183745 -0.072005  -1.169337 -0.960246     0\n",
            "7         -0.585813          0.682553    -1.391472       -0.949853  -0.477498            -0.083669             -0.774449 -1.137769  0.511130  -1.110324 -0.397043     1\n",
            "8         -0.298547          0.291499    -1.288771       -0.382271  -0.307468            -0.657454             -0.865676  0.028261  0.316751  -0.520193 -0.866379     1\n",
            "9         -0.470907         -0.155419     0.457144        2.526589  -0.349975             0.107592              1.688677  0.558274  0.251958   0.837107  0.072294     0\n",
            "10        -0.930531          0.291499    -0.980669       -0.524166   0.202621            -0.083669              0.563545 -0.448752 -0.201591  -0.697233 -1.147981     0\n",
            "11        -0.470907         -0.155419     0.457144        2.526589  -0.349975             0.107592              1.688677  0.558274  0.251958   0.837107  0.072294     0\n",
            "12        -1.562514          0.487026    -1.391472       -0.666062   0.032592             0.011961              0.381091 -1.296773  1.742192  -0.815259 -0.490910     0\n",
            "13        -0.298547          0.459094     0.097691       -0.666062   0.563935            -0.657454             -0.531178  0.346269 -0.331177   5.322101 -1.241848     0\n",
            "14         0.333436          0.514959    -0.467164        0.894790   1.881666             3.454669              2.996263  0.982285 -0.979104   1.309212 -1.147981     0\n",
            "15         0.333436          0.514959    -0.415813        0.965737   1.754143             3.359038              3.087490  0.982285 -0.914312   1.604277 -1.147981     0\n",
            "16         0.103624         -1.384443     1.484154       -0.524166   0.096353             1.828946              1.719086  0.081262 -0.072005   0.542042  0.072294     1\n",
            "17        -0.126188          0.179770     0.046341       -0.595114   5.962380             0.011961              0.289865  0.028261 -1.303068   3.669735 -1.054113     0\n",
            "18        -0.528360          0.347364    -0.980669        1.320476  -0.031169            -0.944346             -0.531178  0.346269  0.446337  -0.933285 -1.335715     0\n",
            "19        -0.241094         -1.160984     1.227401       -0.524166   5.388530             0.107592              0.289865  0.081262 -1.756618   2.489473 -1.147981     0\n",
            "20         0.333436         -1.719632     1.073350       -0.524166  -0.222453             1.255161              0.411500  0.028261  0.511130  -0.756246 -0.960246     0\n",
            "21        -0.413454         -0.769931     0.200392       -0.169427  -0.116184             0.681377              0.745999  0.770280  1.353436  -0.048089 -0.678644     0\n",
            "22        -0.241094         -0.546472    -0.313113       -0.666062   0.393905            -0.561823             -0.287906 -0.077742 -0.914312   1.486251 -0.866379     0\n",
            "23         0.103624         -0.211283    -0.826617       -0.169427  -0.073677            -0.657454              0.624363  0.028261 -0.914312  -0.756246 -0.960246     0\n",
            "24        -0.815625         -0.714066    -0.672566       -0.098479  -0.052423             0.490115             -0.196679  0.028261  0.770301  -0.166115 -0.678644     0\n",
            "25        -1.160343         -0.769931    -0.569865       -0.807957  -0.158692            -0.466193             -0.713631 -0.660757  0.187166  -0.579207 -1.054113     0\n",
            "26        -0.413454         -0.658202    -0.159061       -0.524166  -0.158692            -1.135608             -1.078539 -0.289747 -0.201591  -0.402167 -0.866379     0\n",
            "27        -0.241094         -0.546472    -0.313113       -0.666062   0.393905            -0.561823             -0.287906 -0.077742 -0.914312   1.486251 -0.866379     0\n",
            "28        -0.700719          1.017741    -1.391472       -0.453218  -0.158692            -0.179300             -0.348724  0.240266  1.029472  -0.638220 -0.960246     0\n",
            "29        -0.298547          0.654620    -1.391472       -0.382271  -0.116184            -0.753085             -0.926494 -0.183745  0.446337  -0.402167 -0.584777     0\n",
            "...             ...               ...          ...             ...        ...                  ...                   ...       ...       ...        ...       ...   ...\n",
            "1569      -1.217796         -0.099554    -0.672566       -0.453218  -0.668781            -0.083669             -0.379133 -1.476978  1.094265  -0.520193  1.010966     0\n",
            "1570      -1.102890         -0.937525     1.330102       -0.240375   3.029366             0.298854             -0.348724 -1.773786  0.381544   1.604277  1.855771     0\n",
            "1571      -1.102890         -0.825796    -0.672566       -0.240375  -1.051348            -0.083669             -0.652813 -0.851562  0.835093  -0.048089  0.635497     0\n",
            "1572      -0.585813          0.906012     0.251743       -0.240375  -0.392483             1.828946              1.749495 -0.226146  0.122373  -0.874272 -0.866379     0\n",
            "1573      -1.332702          0.291499    -0.364463       -0.098479  -0.264960            -0.083669              0.107411 -1.100668  1.742192   0.069937  1.949639     0\n",
            "1574      -1.562514         -1.216849     2.613864        8.060517  -0.286214             0.681377              1.384587  0.012360  0.511130  -1.051311  0.072294     0\n",
            "1575      -0.470907         -0.043689     0.662546       -0.240375  -0.583766            -0.370562             -0.804858 -1.063567 -0.331177  -0.107102  1.292568     0\n",
            "1576      -0.183641         -1.272714     1.843607       -0.666062  -0.137438             0.011961             -0.531178 -0.459352 -0.072005   0.719081  0.353895     0\n",
            "1577      -1.217796          0.961877    -0.621215        1.817111  -0.243707            -0.274931             -0.591995 -0.279147  1.483021  -0.343154  1.386435     0\n",
            "1578      -0.873078          0.794282    -0.621215       -0.524166   0.648950            -0.274931             -0.804858 -0.713758  0.705508   0.069937  0.823232     0\n",
            "1579      -1.217796          0.179770    -0.929318       -0.595114  -0.732542             0.777007             -0.439951 -1.445177  1.483021  -0.343154  0.823232     0\n",
            "1580      -0.528360         -0.993390     0.303093       -0.098479  -0.413736            -0.657454             -0.622404 -1.084768  0.316751  -0.343154  1.386435     0\n",
            "1581      -1.217796          0.179770    -0.929318       -0.595114  -0.732542             0.777007             -0.439951 -1.445177  1.483021  -0.343154  0.823232     0\n",
            "1582      -1.275249          1.045674    -0.877968        0.043416  -0.732542            -0.274931             -0.591995 -1.657183  1.677400  -0.933285  1.386435     0\n",
            "1583      -1.217796         -0.378878     0.097691       -0.311323  -0.286214             1.542054              1.567041 -0.512353  0.122373  -0.225128 -0.584777     0\n",
            "1584      -0.930531         -1.160984     0.867948       -0.098479  -0.562513             0.777007             -0.379133 -1.010566 -0.136798   0.837107  1.104833     1\n",
            "1585      -0.643266         -0.769931     0.867948        0.043416  -0.456244             0.585746              0.046593 -0.957565 -0.072005   1.073160  1.010966     0\n",
            "1586      -0.470907         -1.216849     0.713897       -0.098479  -0.477498             1.733315              0.411500 -0.968165  0.187166   1.132173  0.917099     0\n",
            "1587      -1.447608          0.459094    -0.826617       -0.524166  -0.456244             0.203223             -0.561586 -1.015866  1.547814   0.010924  0.447763     0\n",
            "1588      -0.643266          0.738418     0.303093       -0.027532  -0.413736             1.733315              1.688677 -1.381576 -0.266384   0.719081  2.231240     0\n",
            "1589      -0.987984          1.101539    -0.364463        3.732701  -0.307468             1.255161              0.989271  0.505273 -0.136798  -0.697233 -1.147981     0\n",
            "1590      -1.160343          0.123905    -0.621215       -0.524166  -0.222453             0.968269             -0.348724 -1.911589  0.057580   0.955133  1.104833     0\n",
            "1591      -1.677421          1.185336    -0.929318       -0.595114   0.032592             0.011961             -0.622404 -1.445177  2.325327  -0.579207  1.104833     0\n",
            "1592      -1.160343         -0.099554    -0.723916       -0.169427  -0.243707             1.255161             -0.196679 -0.533554  0.705508   0.542042  0.541630     0\n",
            "1593      -0.873078          0.514959    -0.980669       -0.453218  -0.413736             1.159531             -0.257497 -0.125443  0.705508   0.955133 -0.866379     0\n",
            "1594      -1.217796          0.403229    -0.980669       -0.382271   0.053845             1.542054             -0.075043 -0.978765  0.899886  -0.461180  0.072294     0\n",
            "1595      -1.390155          0.123905    -0.877968       -0.240375  -0.541259             2.211469              0.137820 -0.862162  1.353436   0.601055  0.729364     0\n",
            "1596      -1.160343         -0.099554    -0.723916       -0.169427  -0.243707             1.255161             -0.196679 -0.533554  0.705508   0.542042  0.541630     0\n",
            "1597      -1.390155          0.654620    -0.775267       -0.382271  -0.264960             1.542054             -0.075043 -0.676657  1.677400   0.305990 -0.209308     0\n",
            "1598      -1.332702         -1.216849     1.021999        0.752894  -0.434990             0.203223             -0.135861 -0.666057  0.511130   0.010924  0.541630     0\n",
            "\n",
            "[1599 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks75n8a2GXtN",
        "colab_type": "text"
      },
      "source": [
        "**Spliting the Data:** Also same as yesterday"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRr0YHtMGkQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = df.drop('pass', axis = 1)\n",
        "y= df['pass']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7obuxFtjGXRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcmpjVMgGp2y",
        "colab_type": "text"
      },
      "source": [
        "**`Build the model`**.<br>\n",
        " Play around with the hidden layers. Observe how your results change. Try changing the number of the nodes, or even add more layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIC7Kr4HGn1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDfl-7H-Jmu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense\n",
        "\n",
        "model.add(Dense(32,activation='relu',input_dim=11))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "501OMJhRHWcG",
        "colab_type": "text"
      },
      "source": [
        "**Compile the Model**. Same as yesterday"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM4KqNK7HVwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZWxbjDdHeuq",
        "colab_type": "text"
      },
      "source": [
        "**Train the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOgKADSsHiwg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10234
        },
        "outputId": "b81e6034-f657-404b-eb78-644347750f08"
      },
      "source": [
        "model.fit(x_train, y_train, epochs = 300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1023/1023 [==============================] - 1s 597us/step - loss: 0.5900 - acc: 0.8065\n",
            "Epoch 2/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.4609 - acc: 0.8573\n",
            "Epoch 3/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.3781 - acc: 0.8583\n",
            "Epoch 4/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.3278 - acc: 0.8651\n",
            "Epoch 5/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.3016 - acc: 0.8729\n",
            "Epoch 6/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.2868 - acc: 0.8759\n",
            "Epoch 7/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.2793 - acc: 0.8778\n",
            "Epoch 8/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.2736 - acc: 0.8768\n",
            "Epoch 9/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.2693 - acc: 0.8768\n",
            "Epoch 10/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.2662 - acc: 0.8749\n",
            "Epoch 11/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.2638 - acc: 0.8788\n",
            "Epoch 12/300\n",
            "1023/1023 [==============================] - 0s 51us/step - loss: 0.2606 - acc: 0.8837\n",
            "Epoch 13/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.2590 - acc: 0.8827\n",
            "Epoch 14/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.2574 - acc: 0.8827\n",
            "Epoch 15/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.2547 - acc: 0.8827\n",
            "Epoch 16/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.2534 - acc: 0.8837\n",
            "Epoch 17/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.2522 - acc: 0.8866\n",
            "Epoch 18/300\n",
            "1023/1023 [==============================] - 0s 43us/step - loss: 0.2502 - acc: 0.8866\n",
            "Epoch 19/300\n",
            "1023/1023 [==============================] - 0s 43us/step - loss: 0.2492 - acc: 0.8856\n",
            "Epoch 20/300\n",
            "1023/1023 [==============================] - 0s 59us/step - loss: 0.2477 - acc: 0.8866\n",
            "Epoch 21/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.2459 - acc: 0.8856\n",
            "Epoch 22/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.2450 - acc: 0.8856\n",
            "Epoch 23/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.2438 - acc: 0.8876\n",
            "Epoch 24/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.2424 - acc: 0.8886\n",
            "Epoch 25/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.2415 - acc: 0.8856\n",
            "Epoch 26/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.2404 - acc: 0.8895\n",
            "Epoch 27/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.2400 - acc: 0.8915\n",
            "Epoch 28/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.2384 - acc: 0.8886\n",
            "Epoch 29/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.2370 - acc: 0.8886\n",
            "Epoch 30/300\n",
            "1023/1023 [==============================] - 0s 51us/step - loss: 0.2359 - acc: 0.8895\n",
            "Epoch 31/300\n",
            "1023/1023 [==============================] - 0s 53us/step - loss: 0.2348 - acc: 0.8905\n",
            "Epoch 32/300\n",
            "1023/1023 [==============================] - 0s 57us/step - loss: 0.2333 - acc: 0.8886\n",
            "Epoch 33/300\n",
            "1023/1023 [==============================] - 0s 68us/step - loss: 0.2336 - acc: 0.8895\n",
            "Epoch 34/300\n",
            "1023/1023 [==============================] - 0s 57us/step - loss: 0.2323 - acc: 0.8935\n",
            "Epoch 35/300\n",
            "1023/1023 [==============================] - 0s 61us/step - loss: 0.2327 - acc: 0.8944\n",
            "Epoch 36/300\n",
            "1023/1023 [==============================] - 0s 53us/step - loss: 0.2311 - acc: 0.8983\n",
            "Epoch 37/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.2282 - acc: 0.8925\n",
            "Epoch 38/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.2276 - acc: 0.8983\n",
            "Epoch 39/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.2269 - acc: 0.8974\n",
            "Epoch 40/300\n",
            "1023/1023 [==============================] - 0s 54us/step - loss: 0.2252 - acc: 0.8954\n",
            "Epoch 41/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.2243 - acc: 0.8974\n",
            "Epoch 42/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.2251 - acc: 0.8954\n",
            "Epoch 43/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.2224 - acc: 0.8974\n",
            "Epoch 44/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.2218 - acc: 0.8983\n",
            "Epoch 45/300\n",
            "1023/1023 [==============================] - 0s 51us/step - loss: 0.2200 - acc: 0.9032\n",
            "Epoch 46/300\n",
            "1023/1023 [==============================] - 0s 57us/step - loss: 0.2189 - acc: 0.8993\n",
            "Epoch 47/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.2178 - acc: 0.9032\n",
            "Epoch 48/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.2175 - acc: 0.9052\n",
            "Epoch 49/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.2161 - acc: 0.9032\n",
            "Epoch 50/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.2154 - acc: 0.9042\n",
            "Epoch 51/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.2144 - acc: 0.9052\n",
            "Epoch 52/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.2140 - acc: 0.9022\n",
            "Epoch 53/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.2126 - acc: 0.9032\n",
            "Epoch 54/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.2113 - acc: 0.9091\n",
            "Epoch 55/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.2105 - acc: 0.9071\n",
            "Epoch 56/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.2096 - acc: 0.9081\n",
            "Epoch 57/300\n",
            "1023/1023 [==============================] - 0s 58us/step - loss: 0.2073 - acc: 0.9091\n",
            "Epoch 58/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.2066 - acc: 0.9101\n",
            "Epoch 59/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.2065 - acc: 0.9091\n",
            "Epoch 60/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.2055 - acc: 0.9091\n",
            "Epoch 61/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.2038 - acc: 0.9110\n",
            "Epoch 62/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.2033 - acc: 0.9169\n",
            "Epoch 63/300\n",
            "1023/1023 [==============================] - 0s 65us/step - loss: 0.2020 - acc: 0.9101\n",
            "Epoch 64/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.1983 - acc: 0.9159\n",
            "Epoch 65/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.1971 - acc: 0.9189\n",
            "Epoch 66/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.1966 - acc: 0.9140\n",
            "Epoch 67/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1951 - acc: 0.9159\n",
            "Epoch 68/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.1931 - acc: 0.9208\n",
            "Epoch 69/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.1927 - acc: 0.9218\n",
            "Epoch 70/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.1904 - acc: 0.9238\n",
            "Epoch 71/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.1897 - acc: 0.9198\n",
            "Epoch 72/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1891 - acc: 0.9198\n",
            "Epoch 73/300\n",
            "1023/1023 [==============================] - 0s 53us/step - loss: 0.1881 - acc: 0.9267\n",
            "Epoch 74/300\n",
            "1023/1023 [==============================] - 0s 55us/step - loss: 0.1866 - acc: 0.9218\n",
            "Epoch 75/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1846 - acc: 0.9257\n",
            "Epoch 76/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1837 - acc: 0.9228\n",
            "Epoch 77/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.1832 - acc: 0.9306\n",
            "Epoch 78/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.1821 - acc: 0.9277\n",
            "Epoch 79/300\n",
            "1023/1023 [==============================] - 0s 51us/step - loss: 0.1810 - acc: 0.9286\n",
            "Epoch 80/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.1778 - acc: 0.9296\n",
            "Epoch 81/300\n",
            "1023/1023 [==============================] - 0s 51us/step - loss: 0.1763 - acc: 0.9326\n",
            "Epoch 82/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.1758 - acc: 0.9335\n",
            "Epoch 83/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.1744 - acc: 0.9257\n",
            "Epoch 84/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1740 - acc: 0.9316\n",
            "Epoch 85/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1720 - acc: 0.9316\n",
            "Epoch 86/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.1718 - acc: 0.9355\n",
            "Epoch 87/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.1692 - acc: 0.9326\n",
            "Epoch 88/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.1690 - acc: 0.9316\n",
            "Epoch 89/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.1692 - acc: 0.9345\n",
            "Epoch 90/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.1663 - acc: 0.9335\n",
            "Epoch 91/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.1654 - acc: 0.9365\n",
            "Epoch 92/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.1653 - acc: 0.9355\n",
            "Epoch 93/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.1652 - acc: 0.9355\n",
            "Epoch 94/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.1619 - acc: 0.9404\n",
            "Epoch 95/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.1616 - acc: 0.9404\n",
            "Epoch 96/300\n",
            "1023/1023 [==============================] - 0s 42us/step - loss: 0.1583 - acc: 0.9443\n",
            "Epoch 97/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.1591 - acc: 0.9384\n",
            "Epoch 98/300\n",
            "1023/1023 [==============================] - 0s 54us/step - loss: 0.1562 - acc: 0.9394\n",
            "Epoch 99/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.1557 - acc: 0.9365\n",
            "Epoch 100/300\n",
            "1023/1023 [==============================] - 0s 59us/step - loss: 0.1559 - acc: 0.9394\n",
            "Epoch 101/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.1533 - acc: 0.9404\n",
            "Epoch 102/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.1530 - acc: 0.9384\n",
            "Epoch 103/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1520 - acc: 0.9413\n",
            "Epoch 104/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.1494 - acc: 0.9413\n",
            "Epoch 105/300\n",
            "1023/1023 [==============================] - 0s 55us/step - loss: 0.1495 - acc: 0.9453\n",
            "Epoch 106/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.1479 - acc: 0.9433\n",
            "Epoch 107/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1470 - acc: 0.9443\n",
            "Epoch 108/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.1465 - acc: 0.9443\n",
            "Epoch 109/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.1438 - acc: 0.9453\n",
            "Epoch 110/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1446 - acc: 0.9443\n",
            "Epoch 111/300\n",
            "1023/1023 [==============================] - 0s 57us/step - loss: 0.1429 - acc: 0.9492\n",
            "Epoch 112/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.1417 - acc: 0.9472\n",
            "Epoch 113/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1406 - acc: 0.9453\n",
            "Epoch 114/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.1393 - acc: 0.9443\n",
            "Epoch 115/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.1384 - acc: 0.9472\n",
            "Epoch 116/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.1362 - acc: 0.9511\n",
            "Epoch 117/300\n",
            "1023/1023 [==============================] - 0s 63us/step - loss: 0.1354 - acc: 0.9472\n",
            "Epoch 118/300\n",
            "1023/1023 [==============================] - 0s 43us/step - loss: 0.1370 - acc: 0.9501\n",
            "Epoch 119/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1358 - acc: 0.9511\n",
            "Epoch 120/300\n",
            "1023/1023 [==============================] - 0s 43us/step - loss: 0.1339 - acc: 0.9531\n",
            "Epoch 121/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1338 - acc: 0.9511\n",
            "Epoch 122/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.1319 - acc: 0.9531\n",
            "Epoch 123/300\n",
            "1023/1023 [==============================] - 0s 64us/step - loss: 0.1292 - acc: 0.9531\n",
            "Epoch 124/300\n",
            "1023/1023 [==============================] - 0s 43us/step - loss: 0.1292 - acc: 0.9560\n",
            "Epoch 125/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1285 - acc: 0.9531\n",
            "Epoch 126/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.1275 - acc: 0.9550\n",
            "Epoch 127/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1282 - acc: 0.9560\n",
            "Epoch 128/300\n",
            "1023/1023 [==============================] - 0s 59us/step - loss: 0.1251 - acc: 0.9531\n",
            "Epoch 129/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1248 - acc: 0.9531\n",
            "Epoch 130/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.1235 - acc: 0.9560\n",
            "Epoch 131/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.1213 - acc: 0.9580\n",
            "Epoch 132/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.1223 - acc: 0.9550\n",
            "Epoch 133/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1219 - acc: 0.9550\n",
            "Epoch 134/300\n",
            "1023/1023 [==============================] - 0s 53us/step - loss: 0.1196 - acc: 0.9599\n",
            "Epoch 135/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.1186 - acc: 0.9541\n",
            "Epoch 136/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.1202 - acc: 0.9580\n",
            "Epoch 137/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.1159 - acc: 0.9570\n",
            "Epoch 138/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.1170 - acc: 0.9560\n",
            "Epoch 139/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.1142 - acc: 0.9541\n",
            "Epoch 140/300\n",
            "1023/1023 [==============================] - 0s 51us/step - loss: 0.1137 - acc: 0.9599\n",
            "Epoch 141/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.1134 - acc: 0.9599\n",
            "Epoch 142/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.1133 - acc: 0.9570\n",
            "Epoch 143/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1112 - acc: 0.9560\n",
            "Epoch 144/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.1089 - acc: 0.9599\n",
            "Epoch 145/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.1101 - acc: 0.9589\n",
            "Epoch 146/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.1110 - acc: 0.9599\n",
            "Epoch 147/300\n",
            "1023/1023 [==============================] - 0s 67us/step - loss: 0.1069 - acc: 0.9589\n",
            "Epoch 148/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.1064 - acc: 0.9589\n",
            "Epoch 149/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.1055 - acc: 0.9629\n",
            "Epoch 150/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.1053 - acc: 0.9599\n",
            "Epoch 151/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.1060 - acc: 0.9619\n",
            "Epoch 152/300\n",
            "1023/1023 [==============================] - 0s 60us/step - loss: 0.1034 - acc: 0.9609\n",
            "Epoch 153/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.1014 - acc: 0.9609\n",
            "Epoch 154/300\n",
            "1023/1023 [==============================] - 0s 43us/step - loss: 0.1022 - acc: 0.9589\n",
            "Epoch 155/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.1012 - acc: 0.9599\n",
            "Epoch 156/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.0986 - acc: 0.9638\n",
            "Epoch 157/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0972 - acc: 0.9648\n",
            "Epoch 158/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0994 - acc: 0.9619\n",
            "Epoch 159/300\n",
            "1023/1023 [==============================] - 0s 55us/step - loss: 0.0974 - acc: 0.9668\n",
            "Epoch 160/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0961 - acc: 0.9619\n",
            "Epoch 161/300\n",
            "1023/1023 [==============================] - 0s 43us/step - loss: 0.0933 - acc: 0.9638\n",
            "Epoch 162/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0966 - acc: 0.9658\n",
            "Epoch 163/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0925 - acc: 0.9677\n",
            "Epoch 164/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.0913 - acc: 0.9658\n",
            "Epoch 165/300\n",
            "1023/1023 [==============================] - 0s 55us/step - loss: 0.0915 - acc: 0.9648\n",
            "Epoch 166/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.0889 - acc: 0.9687\n",
            "Epoch 167/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.0880 - acc: 0.9668\n",
            "Epoch 168/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.0875 - acc: 0.9677\n",
            "Epoch 169/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.0883 - acc: 0.9658\n",
            "Epoch 170/300\n",
            "1023/1023 [==============================] - 0s 51us/step - loss: 0.0866 - acc: 0.9658\n",
            "Epoch 171/300\n",
            "1023/1023 [==============================] - 0s 60us/step - loss: 0.0856 - acc: 0.9697\n",
            "Epoch 172/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.0850 - acc: 0.9707\n",
            "Epoch 173/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0839 - acc: 0.9677\n",
            "Epoch 174/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.0827 - acc: 0.9707\n",
            "Epoch 175/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.0830 - acc: 0.9707\n",
            "Epoch 176/300\n",
            "1023/1023 [==============================] - 0s 60us/step - loss: 0.0818 - acc: 0.9697\n",
            "Epoch 177/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.0808 - acc: 0.9707\n",
            "Epoch 178/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0799 - acc: 0.9717\n",
            "Epoch 179/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0803 - acc: 0.9707\n",
            "Epoch 180/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0800 - acc: 0.9687\n",
            "Epoch 181/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0778 - acc: 0.9707\n",
            "Epoch 182/300\n",
            "1023/1023 [==============================] - 0s 51us/step - loss: 0.0767 - acc: 0.9746\n",
            "Epoch 183/300\n",
            "1023/1023 [==============================] - 0s 55us/step - loss: 0.0785 - acc: 0.9746\n",
            "Epoch 184/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.0759 - acc: 0.9756\n",
            "Epoch 185/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.0758 - acc: 0.9736\n",
            "Epoch 186/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.0760 - acc: 0.9726\n",
            "Epoch 187/300\n",
            "1023/1023 [==============================] - 0s 51us/step - loss: 0.0734 - acc: 0.9765\n",
            "Epoch 188/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.0737 - acc: 0.9756\n",
            "Epoch 189/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0730 - acc: 0.9746\n",
            "Epoch 190/300\n",
            "1023/1023 [==============================] - 0s 51us/step - loss: 0.0742 - acc: 0.9765\n",
            "Epoch 191/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.0756 - acc: 0.9746\n",
            "Epoch 192/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0733 - acc: 0.9756\n",
            "Epoch 193/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.0707 - acc: 0.9775\n",
            "Epoch 194/300\n",
            "1023/1023 [==============================] - 0s 53us/step - loss: 0.0724 - acc: 0.9707\n",
            "Epoch 195/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0700 - acc: 0.9785\n",
            "Epoch 196/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.0682 - acc: 0.9785\n",
            "Epoch 197/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0684 - acc: 0.9814\n",
            "Epoch 198/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.0726 - acc: 0.9756\n",
            "Epoch 199/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0692 - acc: 0.9756\n",
            "Epoch 200/300\n",
            "1023/1023 [==============================] - 0s 41us/step - loss: 0.0679 - acc: 0.9785\n",
            "Epoch 201/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.0669 - acc: 0.9785\n",
            "Epoch 202/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.0657 - acc: 0.9765\n",
            "Epoch 203/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0658 - acc: 0.9785\n",
            "Epoch 204/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0634 - acc: 0.9824\n",
            "Epoch 205/300\n",
            "1023/1023 [==============================] - 0s 43us/step - loss: 0.0634 - acc: 0.9804\n",
            "Epoch 206/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.0617 - acc: 0.9814\n",
            "Epoch 207/300\n",
            "1023/1023 [==============================] - 0s 64us/step - loss: 0.0628 - acc: 0.9834\n",
            "Epoch 208/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0628 - acc: 0.9824\n",
            "Epoch 209/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.0622 - acc: 0.9824\n",
            "Epoch 210/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0618 - acc: 0.9844\n",
            "Epoch 211/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0632 - acc: 0.9834\n",
            "Epoch 212/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.0590 - acc: 0.9844\n",
            "Epoch 213/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.0586 - acc: 0.9844\n",
            "Epoch 214/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.0583 - acc: 0.9853\n",
            "Epoch 215/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.0577 - acc: 0.9853\n",
            "Epoch 216/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.0581 - acc: 0.9853\n",
            "Epoch 217/300\n",
            "1023/1023 [==============================] - 0s 53us/step - loss: 0.0564 - acc: 0.9873\n",
            "Epoch 218/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.0564 - acc: 0.9853\n",
            "Epoch 219/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.0549 - acc: 0.9853\n",
            "Epoch 220/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.0553 - acc: 0.9824\n",
            "Epoch 221/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0543 - acc: 0.9873\n",
            "Epoch 222/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0535 - acc: 0.9853\n",
            "Epoch 223/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.0532 - acc: 0.9853\n",
            "Epoch 224/300\n",
            "1023/1023 [==============================] - 0s 51us/step - loss: 0.0526 - acc: 0.9883\n",
            "Epoch 225/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.0548 - acc: 0.9853\n",
            "Epoch 226/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.0545 - acc: 0.9834\n",
            "Epoch 227/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.0522 - acc: 0.9863\n",
            "Epoch 228/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.0503 - acc: 0.9863\n",
            "Epoch 229/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.0505 - acc: 0.9883\n",
            "Epoch 230/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0497 - acc: 0.9892\n",
            "Epoch 231/300\n",
            "1023/1023 [==============================] - 0s 53us/step - loss: 0.0511 - acc: 0.9883\n",
            "Epoch 232/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.0487 - acc: 0.9863\n",
            "Epoch 233/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.0494 - acc: 0.9834\n",
            "Epoch 234/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.0486 - acc: 0.9853\n",
            "Epoch 235/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0480 - acc: 0.9892\n",
            "Epoch 236/300\n",
            "1023/1023 [==============================] - 0s 43us/step - loss: 0.0479 - acc: 0.9902\n",
            "Epoch 237/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.0479 - acc: 0.9892\n",
            "Epoch 238/300\n",
            "1023/1023 [==============================] - 0s 43us/step - loss: 0.0468 - acc: 0.9873\n",
            "Epoch 239/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.0457 - acc: 0.9902\n",
            "Epoch 240/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.0461 - acc: 0.9883\n",
            "Epoch 241/300\n",
            "1023/1023 [==============================] - 0s 43us/step - loss: 0.0467 - acc: 0.9873\n",
            "Epoch 242/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.0468 - acc: 0.9873\n",
            "Epoch 243/300\n",
            "1023/1023 [==============================] - 0s 51us/step - loss: 0.0444 - acc: 0.9941\n",
            "Epoch 244/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.0448 - acc: 0.9912\n",
            "Epoch 245/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0449 - acc: 0.9912\n",
            "Epoch 246/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.0431 - acc: 0.9912\n",
            "Epoch 247/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0437 - acc: 0.9902\n",
            "Epoch 248/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.0440 - acc: 0.9873\n",
            "Epoch 249/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.0418 - acc: 0.9892\n",
            "Epoch 250/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.0429 - acc: 0.9912\n",
            "Epoch 251/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0410 - acc: 0.9912\n",
            "Epoch 252/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.0407 - acc: 0.9932\n",
            "Epoch 253/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.0434 - acc: 0.9912\n",
            "Epoch 254/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0399 - acc: 0.9912\n",
            "Epoch 255/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.0393 - acc: 0.9902\n",
            "Epoch 256/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.0390 - acc: 0.9941\n",
            "Epoch 257/300\n",
            "1023/1023 [==============================] - 0s 67us/step - loss: 0.0389 - acc: 0.9912\n",
            "Epoch 258/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0379 - acc: 0.9932\n",
            "Epoch 259/300\n",
            "1023/1023 [==============================] - 0s 43us/step - loss: 0.0376 - acc: 0.9912\n",
            "Epoch 260/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0379 - acc: 0.9922\n",
            "Epoch 261/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.0371 - acc: 0.9912\n",
            "Epoch 262/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0381 - acc: 0.9912\n",
            "Epoch 263/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.0370 - acc: 0.9902\n",
            "Epoch 264/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.0352 - acc: 0.9941\n",
            "Epoch 265/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.0353 - acc: 0.9932\n",
            "Epoch 266/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0366 - acc: 0.9922\n",
            "Epoch 267/300\n",
            "1023/1023 [==============================] - 0s 42us/step - loss: 0.0347 - acc: 0.9932\n",
            "Epoch 268/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.0362 - acc: 0.9932\n",
            "Epoch 269/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.0335 - acc: 0.9941\n",
            "Epoch 270/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.0352 - acc: 0.9932\n",
            "Epoch 271/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0333 - acc: 0.9941\n",
            "Epoch 272/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.0332 - acc: 0.9941\n",
            "Epoch 273/300\n",
            "1023/1023 [==============================] - 0s 44us/step - loss: 0.0349 - acc: 0.9902\n",
            "Epoch 274/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0371 - acc: 0.9912\n",
            "Epoch 275/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0343 - acc: 0.9932\n",
            "Epoch 276/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.0321 - acc: 0.9961\n",
            "Epoch 277/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.0324 - acc: 0.9941\n",
            "Epoch 278/300\n",
            "1023/1023 [==============================] - 0s 63us/step - loss: 0.0310 - acc: 0.9922\n",
            "Epoch 279/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.0323 - acc: 0.9951\n",
            "Epoch 280/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0311 - acc: 0.9941\n",
            "Epoch 281/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.0304 - acc: 0.9941\n",
            "Epoch 282/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.0315 - acc: 0.9971\n",
            "Epoch 283/300\n",
            "1023/1023 [==============================] - 0s 54us/step - loss: 0.0309 - acc: 0.9941\n",
            "Epoch 284/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.0293 - acc: 0.9941\n",
            "Epoch 285/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.0291 - acc: 0.9961\n",
            "Epoch 286/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.0290 - acc: 0.9941\n",
            "Epoch 287/300\n",
            "1023/1023 [==============================] - 0s 50us/step - loss: 0.0284 - acc: 0.9961\n",
            "Epoch 288/300\n",
            "1023/1023 [==============================] - 0s 51us/step - loss: 0.0270 - acc: 0.9961\n",
            "Epoch 289/300\n",
            "1023/1023 [==============================] - 0s 60us/step - loss: 0.0286 - acc: 0.9961\n",
            "Epoch 290/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0271 - acc: 0.9980\n",
            "Epoch 291/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0274 - acc: 0.9971\n",
            "Epoch 292/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.0279 - acc: 0.9961\n",
            "Epoch 293/300\n",
            "1023/1023 [==============================] - 0s 48us/step - loss: 0.0264 - acc: 0.9961\n",
            "Epoch 294/300\n",
            "1023/1023 [==============================] - 0s 52us/step - loss: 0.0274 - acc: 0.9941\n",
            "Epoch 295/300\n",
            "1023/1023 [==============================] - 0s 53us/step - loss: 0.0257 - acc: 0.9961\n",
            "Epoch 296/300\n",
            "1023/1023 [==============================] - 0s 47us/step - loss: 0.0254 - acc: 0.9971\n",
            "Epoch 297/300\n",
            "1023/1023 [==============================] - 0s 49us/step - loss: 0.0252 - acc: 0.9971\n",
            "Epoch 298/300\n",
            "1023/1023 [==============================] - 0s 46us/step - loss: 0.0250 - acc: 0.9961\n",
            "Epoch 299/300\n",
            "1023/1023 [==============================] - 0s 45us/step - loss: 0.0263 - acc: 0.9951\n",
            "Epoch 300/300\n",
            "1023/1023 [==============================] - 0s 54us/step - loss: 0.0242 - acc: 0.9951\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f05c9358be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTac7PyVHqV_",
        "colab_type": "text"
      },
      "source": [
        "**Testing Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQY5b9BONq7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d4246559-57c1-4a10-82cf-6fa95abdcd9a"
      },
      "source": [
        "scores = model.evaluate(x_train, y_train)\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1023/1023 [==============================] - 0s 194us/step\n",
            "[0.025936694520790556, 0.9960899315738025]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-kPtgaSOEy9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "77c6bbb2-4389-4959-ac77-9b282bb35268"
      },
      "source": [
        "scores = model.evaluate(x_val, y_val)\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "256/256 [==============================] - 0s 90us/step\n",
            "[0.5597366206347942, 0.87109375]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}